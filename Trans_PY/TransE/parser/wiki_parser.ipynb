{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"../../../FB15k/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_sum(list1, list2):\n",
    "    s = 0;\n",
    "    for i in xrange(len(list1)):\n",
    "        s += abs(list1[i] - list2[i])\n",
    "    return abs(s)\n",
    "\n",
    "def parseline(line):\n",
    "    vec = []\n",
    "    for i in line.split(\"\\t\"):\n",
    "        if i != \"\\n\":\n",
    "            vec += [float(i)]\n",
    "    return vec\n",
    "def read_vec(path, file_name):\n",
    "    vec = []\n",
    "    f = open(path+file_name, 'r')\n",
    "    data = f.readlines()\n",
    "    f.close()\n",
    "    for i in data:\n",
    "        vec += [parseline(i)]\n",
    "    return vec\n",
    "def read_data(path, file_name):\n",
    "    list1 = []\n",
    "    dict1 = {}\n",
    "    count = 0\n",
    "    f = open(path+file_name, 'r')\n",
    "    data = f.readlines()\n",
    "    f.close()\n",
    "    for i in data:\n",
    "        str1, num = i.split(\"\\t\")\n",
    "        list1 += [str1]\n",
    "        dict1[str1] = count\n",
    "        count += 1\n",
    "    return list1, dict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read result vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_vec = read_vec('../', \"entity2vec.bern\")\n",
    "# relation_vec = read_vec('../', \"relation2vec.bern\")\n",
    "with open('../entity2vec.bern', 'r') as f:\n",
    "    entity_vec = pickle.load(f)\n",
    "with open('../relation2vec.bern', 'r') as f:\n",
    "    relation_vec = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entitylist, entity2id = read_data(data_path, 'entity2id.txt')\n",
    "relationlist, relation2id = read_data(data_path, 'relation2id.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read freebase to wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(data_path+'f2w.nt', \"r\")\n",
    "data = f.readlines()\n",
    "f.close()\n",
    "fb2wiki = {}\n",
    "wiki2fb = {}\n",
    "for i in data:\n",
    "    fb, wiki = i.split(' ')\n",
    "    fb2wiki[fb] = wiki.rstrip()\n",
    "    wiki2fb[wiki.rstrip()] = fb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input search string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input a string:beijing\n",
      "search first id:  (Q956)\n",
      "search first result:  Beijing\n"
     ]
    }
   ],
   "source": [
    "inp_str = raw_input(\"input a string:\")\n",
    "res = requests.get(\"https://www.wikidata.org/w/index.php?search=&search=\"+inp_str+\"&title=Special%3ASearch&go=Go\")\n",
    "soup = BeautifulSoup(res.text,\"html.parser\")\n",
    "for tag in soup.find_all(\"span\", class_=\"wb-itemlink-id\"):\n",
    "\tinp_str = tag.string\n",
    "\tprint \"search first id: \", inp_str\n",
    "\tbreak\n",
    "for tag in soup.find_all(\"span\",  class_=\"wb-itemlink-label\"):\n",
    "\tprint \"search first result: \",tag.string\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = entity2id[wiki2fb[inp_str[1:len(inp_str)-1]]]\n",
    "result = {}\n",
    "K = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calcuate the sum and sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_top_k(ii, target):\n",
    "    candidate_list = {}\n",
    "    print relationlist[ii]\n",
    "    for i in xrange(len(entitylist)):\n",
    "    #\tfor ii in xrange(len(relationvec)):\n",
    "        try:\n",
    "            candidate_list[fb2wiki[entitylist[i]]] = calc_sum(np.add(entity_vec[target], relation_vec[ii]), entity_vec[i])\n",
    "        #\tcandidate_list[fb2wiki[entitylist[i]]] = calc_sum(entityvec[k], entityvec[i])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    # find the top k predictions\n",
    "    count = 0\n",
    "    simlist = []\n",
    "    f = open('cost.txt', 'a+')\n",
    "    f.write(relationlist[ii]+':\\n')\n",
    "    for i in sorted(candidate_list.items(), key= lambda item:item[1]):\n",
    "        simlist += [i[0]]\n",
    "        f.write(i[0] + '\\t' + str(i[1]))\n",
    "        count += 1\n",
    "        if count > K:\n",
    "            break\n",
    "    return simlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## search on wikidata for the meaning of the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_wikidata(simlist):\n",
    "    dict1 = {}\n",
    "    for i in simlist:\n",
    "        res = requests.get(\"https://www.wikidata.org/wiki/\"+i)\n",
    "        res.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(res.text,\"html.parser\")\n",
    "        for tag in soup.find_all(\"span\", class_=\"wikibase-title-label\"):\n",
    "             label = tag.string\n",
    "        for tag in soup.find_all(\"span\", class_=\"wikibase-descriptionview-text\"):\t\n",
    "            desc = tag.string\n",
    "        dict1[label] = desc\n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/people/appointed_role/appointment./people/appointment/appointed_by\n",
      "/location/statistical_region/rent50_2./measurement_unit/dated_money_value/currency\n",
      "/tv/tv_series_episode/guest_stars./tv/tv_guest_role/actor\n",
      "/music/performance_role/track_performances./music/track_contribution/contributor\n",
      "/medicine/disease/prevention_factors\n",
      "/organization/organization_member/member_of./organization/organization_membership/organization\n",
      "/american_football/football_player/receiving./american_football/player_receiving_statistics/season\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "start = 0 \n",
    "if os.path.exists(\"timeout.txt\"):\n",
    "    with open('timeout.txt', 'r') as f:\n",
    "        start = int(f.readlines())\n",
    "for i in range(start, len(relation2id)):\n",
    "    simlist = find_top_k(i, target)\n",
    "    try:\n",
    "        result[relationlist[i]] = search_wikidata(simlist)\n",
    "    except requests.exceptions.ConnectTimeout:\n",
    "        with open('timeout.txt','w') as f:\n",
    "            f.write(start)\n",
    "        with open('result_dict.txt', 'w') as f:\n",
    "            pickle.dump(result, f)\n",
    "        exit(0)\n",
    "    except BaseExceptioin, e:\n",
    "        print e\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
